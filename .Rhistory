x <- to_matrix(nls80, c("exper","tenure","educ")) #only need non-dummy vars
x2 <- x^2
x2vars <- c("exper2","tenure2","educ2")
colnames(x2) <- x2vars
nls80 <- cbind(nls80, x2, inter)
results <- ols(
data = nls80, y_var = "e2",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ", x2vars, intervars), absorb = 0)
nR2 <- results[33,2]*nrow(x)
#p-value
pchisq(q = nR2, df = ncol(x2)+ncol(inter)+8-1, lower.tail=F)
# We reject the null of homoskedasticity: Houston, we have a problem
# Chunk 5: Q.2c Goldfeld-Quandt test for heteroskedastic errors.
#Choose our two subsamples
take = (nrow(nls80)-235)/2
top_tenure <- head(nls80[order(nls80$tenure, decreasing= T),], n = take)
bot_tenure <- head(nls80[order(nls80$tenure, decreasing= F),], n = take)
#Run OLS and record residuals for each
results <- ols(
data = top_tenure, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
e_top <- e
results <- ols(
data = bot_tenure, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
e_bot <- e
#Test statistic
F_stat <- (t(e_bot)%*%e_bot)/(t(e_top)%*%e_top)
F_stat<-F_stat[1,1]
F_stat
pf(q = F_stat, df1 = nrow(top_tenure)-8, df2 =nrow(bot_tenure)-8, lower.tail = F)
#note: n1 − k = n2 − k
#We don't have an issue? Probably. P value is v low.
# Chunk 6: Q.d Breusch  Pagan  Test test for heteroskedastic errors.
#Choose our two subsamples
results <- ols(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
e2 <- e^2
#sigma squared hat
s2<-sum(e2)/dim(nls80)[1]
nls80$e2_normalized<-e2/s2
nls80$e2<-e2
#create our z-vector
z<-c("intercept","exper","tenure","married","south","urban","black","educ")
results <- ols(
data = nls80 , y_var = "e2",
X_vars = z, absorb = 0)
#Change statistic: 1/2 ESS of estimated model
nR2 <- results[9,2]*nrow(nls80)
nR2
pchisq(q = nR2, df = 7, lower.tail=F)
#0.001
# Chunk 7: Q.e White standard errors
#Choose our two subsamples
results <- ols(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
ols_se <- results[1:8,3] #for later
e2 <- e^2
X <- to_matrix(nls80, c("intercept","exper","tenure","married","south","urban","black","educ"))
xx_inv <- solve(t(X) %*% X)
sigma_hat <- lapply(X = 1:nrow(X), FUN = function(i) {
# Define x_i
x_i <- matrix(as.vector(X[i,]), nrow = 1)
# Return x_i' x_i e_i^2
return(t(x_i) %*% x_i * e[i]^2)
}) %>% Reduce(f = "+", x = .)
vcov_white <- xx_inv %*% sigma_hat %*% xx_inv
# test against canned package
library(sandwich)
mod<-lm(lwage ~ exper + tenure + married + south + urban + black + educ, data=nls80)
vcovHC(mod, type = "HC")
## This is right, just need to look at diagonals, compare to regular standard errors.
results <- ols(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
white_se <- sqrt(diag(vcov_white))
# Compare ols_se vs white_se
# Chunk 8: Q. 2.F FGLS
results <- ols(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
#Square residuals
n = 0
while (n<1) { #loop for convergence; not looping here
nls80$e2 <- e^2
#Regress on z
results <- ols(
data = nls80, y_var = "e2",
X_vars = z, absorb = 0)
#Get predicted values and use them as weights
C <- diag(as.vector(1 / sqrt(y_hat)))
# Re-weight y and X (in the function)
results <- fgls(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
print(results)
print(n)
n <- n + 1
}
% ####################################################################
% #
% # PROGRAM: ARE 212 Problem Set 3
% #
% # PROGRAMMERS: Sebastien Annan-Phan, Alejandro Favela, Matthew Tadruno
% #
% # INITIAL DATE: March 23, 2018
% #
% #####################################################################
\documentclass[english, 11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage[normalem]{ulem}
\usepackage{booktabs}
\begin{document}
\title{ARE 212 Problem Set 3}
\author{Sebastien Annan-Phan, Alejandro Favela, Matthew Tadruno}
\maketitle
% # ####################################################
% # Data setup, cleaning and OLS function
% # ####################################################
<< Data setup & defining functions, echo = F, message=F>>=
rm(list=ls())
options(scipen = 999)
# Packages
library(pacman)
library(ggpubr)
library(googledrive)
library(formattable)
p_load(dplyr, haven, readr, gdata, googledrive)
p_load(ggplot2, extrafont, Matrix, reshape, formattable)
p_load(dplyr, lfe, magrittr, ggplot2, viridis, sandwich)
# Directories
if (Sys.getenv("LOGNAME")=="AlexFavela") {
dir_data <- "~/Google Drive/ARE 212 Psets/Pset 3/"
}
if (Sys.getenv("LOGNAME")=="matthewtarduno") {
dir_data <- "~/Google Drive/ARE 212 Psets/Pset 3/"
# If this doesn't work, set your gdrive path
}
if (Sys.getenv("LOGNAME")=="") {
dir_data <- "C:/Users/sphan/Google Drive/ARE 212 Psets/Pset 3/"
}
## Theme
SAP_theme <- theme(
panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
plot.title = element_text(size = 16, face = "bold"), axis.text=element_text(size=14),
axis.title.x=element_text(size=14,face="bold", margin = margin(t = 30, r = 0, b = 0, l = 0)),
axis.title.y=element_text(size=14,face="bold", margin = margin(t = 0, r = 0, b = 0, l = 15)),
axis.text.x=element_text(size = 12)
)
##
# To MATRIX ############################################################################
# Function to convert tibble, data.frame, or tbl_df to matrix ----
to_matrix <- function(the_df, vars) {
# Create a matrix from variables in var
new_mat <- the_df %>%
# Select the columns given in 'vars'
select_(.dots = vars) %>%
# Convert to matrix
as.matrix()
# Return 'new_mat'
return(new_mat)
}
# Simple OLS ################################################################################
b_ols <- function(data, y_var, X_vars) {
# Create the y matrix
y <- to_matrix(the_df = data, vars = y_var)
# Create the X matrix
X <- to_matrix(the_df = data, vars = X_vars)
# Calculate beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
# Return beta_hat
return(beta_hat)
}
# OLS + stats################################################################################
ols <- function(data, y_var, X_vars, absorb) {
# Turn data into matrices
y <- to_matrix(data, y_var)
X <- to_matrix(data, X_vars)
# Calculate n and k for degrees of freedom
n <- nrow(X)
k <- ncol(X)
# Estimate coefficients
b <- b_ols(data, y_var, X_vars)
# Calculate OLS residuals
e <- y - X %*% b
assign("e", e, .GlobalEnv)
# Calculate s^2
s2 <- (t(e) %*% e) / (n-k)
# Update s2 to numeric
s2 %<>% as.numeric()
# Inverse of X'X
XX_inv <- solve(t(X) %*% X)
# Standard error
se <- sqrt(s2 * diag(XX_inv))
t <- b / se
# Adjusted R-sq
y_hat <- y - e
assign("y_hat", y_hat, .GlobalEnv)
SSM_demean <- sum((y_hat - mean(y)) * (y_hat - mean(y)))
SST_demean <- sum((y - mean(y)) * (y - mean(y)))
SSR <- sum(e * e)
R <- 1 - SSR/SST_demean
R <- formattable(R, digits = 3, format = "f")
R_adj <- 1 - ((1 - R) * (n-1)/(n-k-absorb))
R_adj <- formattable(R_adj, digits = 3, format = "f")
# Nice table (data.frame) of results
results <- data.frame(
# The rows have the coef. names
effect = rownames(b),
# Estimated coefficients
coef = formattable(as.vector(b), digits = 3, format = "f"),
# Standard errors
std_error = formattable(as.vector(se), digits = 3, format = "f"),
# T-stat for beta = 0
tstat0 = formattable(as.vector(t), digits = 3, format = "f")
)
rsq <- data.frame(
effect = c("R", "R_adj"),
coef = c(R,R_adj),
std_error = as.numeric(c("","")),
tstat0 = as.numeric(c("",""))
)
results<- rbind(results, rsq)
# Return the results
return(results)
}
# FGLS + stats################################################################################
fgls <- function(data, y_var, X_vars, absorb) {
# Turn data into matrices
y <- to_matrix(data, y_var)
y <- C %*% y
X <- to_matrix(data, X_vars)
X <- C %*% X
# Calculate n and k for degrees of freedom
n <- nrow(X)
k <- ncol(X)
# Estimate coefficients
data_new<-as.data.frame(cbind(y,X))
b <- b_ols(data_new, y_var, X_vars)
# Calculate OLS residuals
e <- y - X %*% b
assign("e", e, .GlobalEnv)
# Calculate s^2
s2 <- (t(e) %*% e) / (n-k)
# Update s2 to numeric
s2 %<>% as.numeric()
# Inverse of X'X
XX_inv <- solve(t(X) %*% X)
# Standard error
se <- sqrt(s2 * diag(XX_inv))
t <- b / se
# Adjusted R-sq
y_hat <- y - e
assign("y_hat", y_hat, .GlobalEnv)
SSM_demean <- sum((y_hat - mean(y)) * (y_hat - mean(y)))
SST_demean <- sum((y - mean(y)) * (y - mean(y)))
SSR <- sum(e * e)
R <- 1 - SSR/SST_demean
R <- formattable(R, digits = 3, format = "f")
R_adj <- 1 - ((1 - R) * (n-1)/(n-k-absorb))
R_adj <- formattable(R_adj, digits = 3, format = "f")
# Nice table (data.frame) of results
results <- data.frame(
# The rows have the coef. names
effect = rownames(b),
# Estimated coefficients
coef = formattable(as.vector(b), digits = 3, format = "f"),
# Standard errors
std_error = formattable(as.vector(se), digits = 3, format = "f"),
# T-stat for beta = 0
tstat0 = formattable(as.vector(t), digits = 3, format = "f")
)
rsq <- data.frame(
effect = c("R", "R_adj"),
coef = c(R,R_adj),
std_error = as.numeric(c("","")),
tstat0 = as.numeric(c("",""))
)
results<- rbind(results, rsq)
# Return the results
return(results)
}
# t stats################################################################################
t_stat <- function(data, y_var, X_vars, gamma) {
# Turn data into matrices
y <- to_matrix(data, y_var)
X <- to_matrix(data, X_vars)
# Add intercept if requested
# Calculate n and k for degrees of freedom
n <- nrow(X)
k <- ncol(X)
# Estimate coefficients
b <- b_ols(data, y_var, X_vars)
# Calculate OLS residuals
e <- y - X %*% b
# Calculate s^2
s2 <- (t(e) %*% e) / (n-k)
# Force s2 to numeric
s2 %<>% as.numeric()
# Inverse of X'X
XX_inv <- solve(t(X) %*% X)
# Standard error
se <- sqrt(s2 * diag(XX_inv))
# Vector of _t_ statistics
t_stats <- (b - gamma) / se
# Return the _t_ statistics
return(t_stats)
}
############################################################################################
@
\newpage
\noindent \section*{Question 1} Read the data into R. Plot the series and make sure your data are read in correctly. \\
<<Q.1 Read data into R and plot series., echo = T, message=F, results="hide", fig.keep="none" , size="footnotesize">>=
nls80 <- read_csv(paste0(dir_data, "nls80.csv"))
#Sort data by wage
nls80 <- nls80[order(wage),]
#Scatterplot series to identify outliers
clist <- list("exper","tenure","married","south","urban","black","educ")
for (var in clist){
attach(nls80)
y_var = get(var)
gph = ggplot() +
geom_point(aes_(x = lwage, y = y_var),
col = "maroon",  alpha = .5, size = 2) +
ggtitle(paste0(var)) + ylab(var) + xlab("Log of Wafe") + SAP_theme
assign(paste0("gph_",var), gph)
}
detach(nls80)
variable_plot1<-ggarrange(
gph_exper, gph_tenure, gph_educ, ncol = 3, nrow = 1, align = "hv")
variable_plot2<-ggarrange(
gph_married, gph_south, gph_urban, gph_black, ncol = 2, nrow = 2, align = "hv")
remove(gph_exper, gph_tenure, gph_educ, gph_married, gph_south, gph_urban, gph_black)
#Data cleaning
nls80$intercept <- 1
@
\noindent \section*{Question 2} Exploring the issue of heteroskedasticity \\
\noindent \subsection*{(a) Estimate the model via least squares.}
<<Q.2a Estimate model, echo = T, results="hide", size="footnotesize">>=
results <- ols(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban", "black","educ"), absorb = 0)
@
\noindent \subsection*{(b) Conduct a White test for heteroskedastic errors.  Use levels, interactions and second order terms only.Do we have a problem?}
<<Q.2b White test for heteroskedastic errors., echo = T, results="hide", size="footnotesize">>=
#Generate squared residuals
nls80$e <- e
nls80$e2 <- (e)^2
#Plot residuals^2 to get idea
scatter_e2<-ggplot(nls80, aes(y=e2, x=lwage)) +
geom_point(size=2, shape=19, color="darkblue", alpha=0.3)+
labs(
x= "Log of Wage",
y= "residuals"
)
#Generate interaction terms
x <- to_matrix(nls80, c("exper","tenure","married","south","urban","black","educ"))
inter <- t(apply(x, 1, combn, 2, prod))
intervars <- c("exper_tenure", "exper_married", "exper_south", "exper_urban","exper_black", "exper_educ",  "tenure_married", "tenure_south", "tenure_urban","tenure_black", "tenure_educ", "married_south", "married_urban","married_black", "married_educ","south_urban", "south_black", "south_educ", "urban_black","urban_educ", "black_educ")
colnames(inter) <- intervars
#Generate squared vars
x <- to_matrix(nls80, c("exper","tenure","educ")) #only need non-dummy vars
x2 <- x^2
x2vars <- c("exper2","tenure2","educ2")
colnames(x2) <- x2vars
nls80 <- cbind(nls80, x2, inter)
results <- ols(
data = nls80, y_var = "e2",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ", x2vars, intervars), absorb = 0)
nR2 <- results[33,2]*nrow(x)
#p-value
pchisq(q = nR2, df = ncol(x2)+ncol(inter)+8-1, lower.tail=F)
# We reject the null of homoskedasticity: Houston, we have a problem
@
\noindent \subsection*{(c) Conduct a Goldfeld-Quandt Test for heteroskedastic errors.  (Use the tenure variable, leaving out the 235 observations in the middle.)  Do we have a problem?}
<<Q.2c Goldfeld-Quandt test for heteroskedastic errors., echo = T, results="hide", size="footnotesize">>=
#Choose our two subsamples
take = (nrow(nls80)-235)/2
top_tenure <- head(nls80[order(nls80$tenure, decreasing= T),], n = take)
bot_tenure <- head(nls80[order(nls80$tenure, decreasing= F),], n = take)
#Run OLS and record residuals for each
results <- ols(
data = top_tenure, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
e_top <- e
results <- ols(
data = bot_tenure, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
e_bot <- e
#Test statistic
F_stat <- (t(e_bot)%*%e_bot)/(t(e_top)%*%e_top)
F_stat<-F_stat[1,1]
F_stat
pf(q = F_stat, df1 = nrow(top_tenure)-8, df2 =nrow(bot_tenure)-8, lower.tail = F)
#note: n1 − k = n2 − k
#We don't have an issue? Probably. P value is v low.
@
<<Q.d Breusch  Pagan  Test test for heteroskedastic errors., echo = T, results="hide", size="footnotesize">>=
#Choose our two subsamples
results <- ols(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
e2 <- e^2
#sigma squared hat
s2<-sum(e2)/dim(nls80)[1]
nls80$e2_normalized<-e2/s2
#create our z-vector
z<-c("intercept","exper","tenure","married","south","urban","black","educ")
results <- ols(
data = nls80 , y_var = "e2_normalized",
X_vars = z, absorb = 0)
#Change statistic: 1/2 ESS of estimated model
nR2 <- results[9,2]*nrow(nls80)
pchisq(q = nR2, df = 7, lower.tail=F)
#0.001
@
<<Q.e White standard errors, echo = T, results="hide", size="footnotesize">>=
#Choose our two subsamples
results <- ols(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
ols_se <- results[1:8,3] #for later
e2 <- e^2
X <- to_matrix(nls80, c("intercept","exper","tenure","married","south","urban","black","educ"))
xx_inv <- solve(t(X) %*% X)
sigma_hat <- lapply(X = 1:nrow(X), FUN = function(i) {
# Define x_i
x_i <- matrix(as.vector(X[i,]), nrow = 1)
# Return x_i' x_i e_i^2
return(t(x_i) %*% x_i * e[i]^2)
}) %>% Reduce(f = "+", x = .)
vcov_white <- xx_inv %*% sigma_hat %*% xx_inv
# test against canned package
library(sandwich)
mod<-lm(lwage ~ exper + tenure + married + south + urban + black + educ, data=nls80)
vcovHC(mod, type = "HC")
## This is right, just need to look at diagonals, compare to regular standard errors.
results <- ols(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
white_se <- sqrt(diag(vcov_white))
# Compare ols_se vs white_se
@
<<Q. 2.F FGLS, echo = T, results="hide", size="footnotesize">>=
results_ols <- ols(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
#Square residuals
n = 0
while (n<1) { #loop for convergence; not looping here
nls80$e2 <- e^2
#Regress on z
results <- ols(
data = nls80, y_var = "e2",
X_vars = z, absorb = 0)
#Get predicted values and use them as weights
C <- diag(as.vector(1 / sqrt(y_hat)))
# Re-weight y and X (in the function)
results_fgls <- fgls(
data = nls80, y_var = "lwage",
X_vars = c("intercept","exper","tenure","married","south","urban","black","educ"), absorb = 0)
print(results)
print(n)
n <- n + 1
}
@
View(results_ols)
results_ols[4,3]
results_ols[4,2]
100·[exp(results_ols[4,2])]·se(results_ols[4,3])
100*[exp(results_ols[4,2])]*results_ols[4,3]
100*exp(results_ols[4,2])*results_ols[4,3]
theta1<- 100*(exp(results_ols[4,2])−1)
theta1<- 100*(exp(results_ols[4,2])-1)
theta1
theta1<- 100*(exp(results_fgls[4,2])-1)
asymptotic_se<-100*exp(results_fgls[4,2])*results_ols[4,3]
theta1
asymptotic_se
theta1<- 100*(exp(results_fgls[4,2])-1)
asymptotic_se<-100*exp(results_fgls[4,2])*results_fgls[4,3]
asymptotic_se
theta1
setwd("/Users/matthewtarduno/Desktop/python screenshot")
library(data.table)
library(ggplot2)
library(ggthemes)
library(dplyr)
library(chron)
library(wesanderson)
library(reshape2)
library(webshot)
webshot("https://www.r-project.org/", "r.png")
install.packages("webshot")
library(webshot)
webshot("https://www.r-project.org/", "r.png")
webshot("http://www.dot.ca.gov/d4/d4cameras/ct-cam-pop-E80_at_Powells_St_Or.html", "r.png")
